import { GetObjectCommand, S3, S3Client } from "@aws-sdk/client-s3";
import { Upload } from "@aws-sdk/lib-storage";
import pl from "nodejs-polars";
import { createReadStream } from "node:fs";
import { chdir } from "node:process";
import { StepsS3CopyInvokeSettings } from "../../src/steps-s3-copy-construct";
import { StepsS3CopyInvokeArguments } from "../../src/steps-s3-copy-input";
import { tmpNameSync } from "tmp";
import path = require("node:path/posix");

interface LambdaEvent {
  // settings for the overall Steps invocation
  invokeSettings: StepsS3CopyInvokeSettings;

  // arguments passed to the overall Steps invocation
  invokeArguments: StepsS3CopyInvokeArguments;

  // the manifest bucket and key are definitions of where in S3
  // results from the previous DistributedMaps operations have gone
  // in this particular case - this is a result that documents HEAD level details
  // of every object we want to copy
  headObjectsResults: {
    manifestBucket: string;
    manifestAbsoluteKey: string;
  };
}

// we should pass this in from above
// set to 5 MiB as that is the definitional minimum size of a multipart part
const SIZE_THRESHOLD_BYTES = 5 * 1024 * 1024;

/**
 * A handler that processes the list/head of all the objects that we are
 * attempting to copy - and creates sub-list suitable for input to
 * other stages. For example, create a list of objects (in S3 as JSONL)
 * that need to be thawed to pass to the thawing stage.
 *
 * @param event
 */
export async function handler(event: LambdaEvent) {
  // debug input event
  console.debug("coordinateCopy()");
  console.debug(JSON.stringify(event, null, 2));

  // polars writes out files even in intermediate steps so we need to live in a writable folder
  chdir("/tmp");

  const client = new S3Client({});

  // the manifest.json is generated by an AWS Steps DISTRIBUTED map and shows the results
  // of all the individual map run parts
  const getManifestCommand = new GetObjectCommand({
    Bucket: event.headObjectsResults.manifestBucket,
    Key: event.headObjectsResults.manifestAbsoluteKey,
  });

  const getManifestResult = await client.send(getManifestCommand);

  const getManifestContent = await getManifestResult.Body.transformToString();

  // A sample manifest
  // {"DestinationBucket":"elsa-data-tmp",
  // "MapRunArn":"arn:aws:states:ap-southeast-2:12345678:mapRun:CopyOutSta",
  // "ResultFiles":{
  //     "FAILED":[],
  //     "PENDING":[],
  //     "SUCCEEDED":[{"Key":"copy-out-test-working/a6faea86c066cd90/1-objects-to-copy.tsv/0c17ffd6-e8ad-44c0-a65b-a8b721007241/SUCCEEDED_0.json",
  //                   "Size":2887}]}}

  const manifest = JSON.parse(getManifestContent);

  console.debug(JSON.stringify(manifest, null, 2));

  const rf = manifest["ResultFiles"];

  if (!rf)
    throw new Error(
      "AWS Steps Distributed map manifest.json is missing ResultFiles",
    );

  const pending = rf["PENDING"];
  const failed = rf["FAILED"];
  const succeeded = rf["SUCCEEDED"];

  if (
    !Array.isArray(pending) ||
    !Array.isArray(failed) ||
    !Array.isArray(succeeded)
  )
    throw new Error(
      "AWS Steps Distributed map manifest.json is missing an expected array for PENDING, FAILED or SUCCEEDED",
    );

  if (pending.length > 0)
    throw new Error(
      "AWS Steps Distributed map manifest.json indicates there are PENDING results which is not a state we are expecting",
    );

  if (failed.length > 0)
    throw new Error("Copy is meant to succeed - but it had failed results");

  for (const s of succeeded) {
    console.debug(JSON.stringify(s, null, 2));

    const getSuccessResult = await client.send(
      new GetObjectCommand({
        Bucket: event.headObjectsResults.manifestBucket,
        Key: s["Key"],
      }),
    );

    const getSuccessContent = await getSuccessResult.Body.transformToString();

    const df = pl.readJSON(getSuccessContent, {
      // we infer the schema from the entire table
      inferSchemaLength: null,
      format: "lines",
    });

    const stats = await computeStats(df);

    // if we are doing a dry run - then we have done all we want to do (head objects and stats) - pass
    // an empty list of objects to the actual copiers
    const smallDf = event.invokeArguments.dryRun
      ? df.filter(false)
      : df.filter(pl.col("size").ltEq(SIZE_THRESHOLD_BYTES));
    const largeDf = event.invokeArguments.dryRun
      ? df.filter(false)
      : df.filter(pl.col("size").gt(SIZE_THRESHOLD_BYTES));

    await createJsonlFromDataFrame(
      event.headObjectsResults.manifestBucket,
      event.headObjectsResults.manifestAbsoluteKey,
      smallDf,
      "small",
    );
    await createJsonlFromDataFrame(
      event.headObjectsResults.manifestBucket,
      event.headObjectsResults.manifestAbsoluteKey,
      largeDf,
      "large",
    );

    return {
      stats: stats,
      copySets: {
        small: {
          bucket: event.headObjectsResults.manifestBucket,
          key: smallKey,
        },
        large: {
          bucket: event.headObjectsResults.manifestBucket,
          key: largeKey,
        },
      },
    };
  }
}

/**
 *
 * @param originalBucket
 * @param originalKey
 * @param df
 * @param newName
 */
async function createJsonlFromDataFrame(
  originalBucket: string,
  originalKey: string,
  df: pl.DataFrame,
  newName: string,
) {
  const tmpName = tmpNameSync();

  // write the frame locally on disk in our jsonl format
  df.writeJSON(tmpName, { format: "lines" });

  // now upload to S3 in a location adjacent to the original manifest that was passed to us
  const originalParts = path.parse(originalKey);
  originalParts.base = newName + ".jsonl";
  const newKey = path.format(originalParts);

  await uploadFile(tmpName, originalBucket, newKey);
}

/**
 * Given a dataframe of objects to copy - compute some useful stats.
 *
 * @param df
 */
async function computeStats(df: pl.DataFrame) {
  return {
    objectToCopyCount: df.getColumn("sourceKey").len(),
    objectToCopySizeInBytes: df.getColumn("size").sum(),
  };
}

/**
 * Functionality to upload our new JSONL objects into S3.
 *
 * @param filePath
 * @param bucket
 * @param key
 */
async function uploadFile(filePath: string, bucket: string, key: string) {
  const fileStream = createReadStream(filePath);

  const parallelUploads3 = new Upload({
    client: new S3({}) || new S3Client({}),
    params: {
      Bucket: bucket,
      Key: key,
      Body: fileStream,
    },
    // additional optional fields show default values below:
    // (optional) concurrency configuration
    // queueSize: 4,
    // (optional) size of each part, in bytes, at least 5MB
    partSize: 1024 * 1024 * 5,
    // (optional) when true, do not automatically call AbortMultipartUpload when
    // a multipart upload fails to complete. You should then manually handle
    // the leftover parts.
    leavePartsOnError: false,
  });
  parallelUploads3.on("httpUploadProgress", (progress) => {
    console.log(progress);
  });
  await parallelUploads3.done();
}
