import { GetObjectCommand, S3, S3Client } from "@aws-sdk/client-s3";
import { Upload } from "@aws-sdk/lib-storage";

import pl from "nodejs-polars";
import { createReadStream } from "node:fs";
import { chdir } from "node:process";
import path = require("node:path/posix");

interface InvokeEvent {
  // the manifest bucket and key are definitions of where in S3
  // results from the previous DistributedMaps operations have gone
  // in this particular case - this is a result that documents HEAD level details
  // of every object we want to copy
  manifestBucket: string;
  manifestAbsoluteKey: string;

  workingBucket: string;
}

// we should pass this in from above
const SIZE_THRESHOLD_BYTES = 16 * 1024 * 1024;

/**
 * A handler that processes the list/head of all the objects that we are
 * attempting to copy - and creates sub-list suitable for input to
 * other stages. For example, create a list of objects (in S3 as JSONL)
 * that need to be thawed to pass to the thawing stage.
 *
 * @param event
 */
export async function handler(event: InvokeEvent) {
  // debug input event
  console.debug("coordinateCopy()");
  console.debug(JSON.stringify(event, null, 2));

  // polars writes out files even in intermediate steps so we need to live in a writable folder
  chdir("/tmp");

  const client = new S3Client({});

  // the manifest.json is generated by an AWS Steps DISTRIBUTED map and shows the results
  // of all the individual map run parts
  const getManifestCommand = new GetObjectCommand({
    Bucket: event.manifestBucket,
    Key: event.manifestAbsoluteKey,
  });

  const getManifestResult = await client.send(getManifestCommand);

  const getManifestContent = await getManifestResult.Body.transformToString();

  // A sample manifest
  // {"DestinationBucket":"elsa-data-tmp",
  // "MapRunArn":"arn:aws:states:ap-southeast-2:12345678:mapRun:CopyOutSta",
  // "ResultFiles":{
  //     "FAILED":[],
  //     "PENDING":[],
  //     "SUCCEEDED":[{"Key":"copy-out-test-working/a6faea86c066cd90/1-objects-to-copy.tsv/0c17ffd6-e8ad-44c0-a65b-a8b721007241/SUCCEEDED_0.json",
  //                   "Size":2887}]}}

  const manifest = JSON.parse(getManifestContent);

  console.debug(JSON.stringify(manifest, null, 2));

  const rf = manifest["ResultFiles"];

  if (!rf)
    throw new Error(
      "AWS Steps Distributed map manifest.json is missing ResultFiles",
    );

  const pending = rf["PENDING"];
  const failed = rf["FAILED"];
  const succeeded = rf["SUCCEEDED"];

  if (
    !Array.isArray(pending) ||
    !Array.isArray(failed) ||
    !Array.isArray(succeeded)
  )
    throw new Error(
      "AWS Steps Distributed map manifest.json is missing an expected array for PENDING, FAILED or SUCCEEDED",
    );

  if (pending.length > 0)
    throw new Error(
      "AWS Steps Distributed map manifest.json indicates there are PENDING results which is not a state we are expecting",
    );

  if (failed.length > 0)
    throw new Error("Copy is meant to succeed - but it had failed results");

  for (const s of succeeded) {
    console.debug(JSON.stringify(s, null, 2));

    const getSuccessResult = await client.send(
      new GetObjectCommand({
        Bucket: event.manifestBucket,
        Key: s["Key"],
      }),
    );

    const getSuccessContent = await getSuccessResult.Body.transformToString();

    const df = pl.readJSON(getSuccessContent, {
      // we infer the schema from the entire table
      inferSchemaLength: null,
      format: "lines",
    });

    const manifestParts = path.parse(event.manifestAbsoluteKey);

    const SMALL_NAME = "small.jsonl";
    const LARGE_NAME = "large.jsonl";

    const smallDf = df.filter(pl.col("size").gt(SIZE_THRESHOLD_BYTES));
    const largeDf = df.filter(pl.col("size").ltEq(SIZE_THRESHOLD_BYTES));

    smallDf.writeJSON(SMALL_NAME, { format: "lines" });
    manifestParts.base = SMALL_NAME;
    const smallKey = path.format(manifestParts);
    await uploadFile(SMALL_NAME, event.manifestBucket, smallKey);

    largeDf.writeJSON(LARGE_NAME, { format: "lines" });
    manifestParts.base = LARGE_NAME;
    const largeKey = path.format(manifestParts);
    await uploadFile(LARGE_NAME, event.manifestBucket, largeKey);

    return {
      small: {
        bucket: event.manifestBucket,
        key: smallKey,
      },
      large: {
        bucket: event.manifestBucket,
        key: largeKey,
      },
    };
  }
}

async function uploadFile(filePath: string, bucket: string, key: string) {
  const fileStream = createReadStream(filePath);

  const parallelUploads3 = new Upload({
    client: new S3({}) || new S3Client({}),
    params: {
      Bucket: bucket,
      Key: key,
      Body: fileStream,
    },
    // additional optional fields show default values below:
    // (optional) concurrency configuration
    // queueSize: 4,
    // (optional) size of each part, in bytes, at least 5MB
    partSize: 1024 * 1024 * 5,
    // (optional) when true, do not automatically call AbortMultipartUpload when
    // a multipart upload fails to complete. You should then manually handle
    // the leftover parts.
    leavePartsOnError: false,
  });
  parallelUploads3.on("httpUploadProgress", (progress) => {
    console.log(progress);
  });
  await parallelUploads3.done();
}
