import {
  GetObjectCommand,
  PutObjectCommand,
  S3Client,
} from "@aws-sdk/client-s3";
import { basename } from "path/posix";
import { stringify } from "csv-stringify/sync";
import { URL } from "url";
import { createHtmlReport } from "./create-html-report.ts";

interface InvokeEvent {
  rcloneResultsLarge: {
    manifestBucket: string;
    manifestKey: string;
  };
  rcloneResultsSmall: {
    manifestBucket: string;
    manifestKey: string;
  };
  rcloneResultsNeedThawSmall: {
    manifestBucket: string;
    manifestKey: string;
  };
  rcloneResultsNeedThawLarge: {
    manifestBucket: string;
    manifestKey: string;
  };
  destinationBucket: string;
  destinationPrefixKey: string;
  destinationEndCopyRelativeKey: string;
  workingBucket: string;
  includeCopyReport?: boolean;
  retainCopyReportS3Uri?: string;
}

type TransferStatus = "ERROR" | "ALREADYCOPIED" | "COPIED";

interface FileResult {
  name: string;
  status: TransferStatus;
  speed: number;
  message: string | number;
  destination: string;
  bytesTransferred: number;
  elapsedSeconds: number;
  // source: string;
  // copyMode: string;
}
// Parses an S3 URI and returns the bucket and key.
function parseS3Uri(s3Uri: string): {
  bucket: string | null;
  key: string | null;
} {
  try {
    // The URL constructor can parse the 's3:' scheme.
    const parsedUrl = new URL(s3Uri);

    // The 'host' part is the bucket name (e.g., 'my-bucket').
    const bucket = parsedUrl.host;

    // The 'pathname' part is the object key (e.g., '/path/to/object.txt').
    // We remove the leading slash for the key.
    const key = parsedUrl.pathname.startsWith("/")
      ? parsedUrl.pathname.substring(1)
      : parsedUrl.pathname;

    return { bucket, key };
  } catch (error) {
    console.error(`Invalid S3 URI: ${s3Uri}`, error);
    return { bucket: null, key: null };
  }
}

/**
 * A handler that process the Steps CSV result (which should have a bunch
 * of rclone stats) - and converts that into a CSV report for the
 * copy out destination.
 *
 * @param event
 */
export async function handler(event: InvokeEvent) {
  // debug input event
  console.debug(JSON.stringify(event, null, 2));

  const client = new S3Client({});

  // Each Distributed Map we run (Large, Small, NeedThawSmall, NeedThawLarge) writes a
  // `manifest.json` to S3. The manifest is generated by Step Functions and summarises the
  // per-item result files for that map run (grouped into PENDING / FAILED / SUCCEEDED).
  //
  // Example:
  // {"DestinationBucket":"elsa-data-tmp",
  // "MapRunArn":"arn:aws:states:ap-southeast-2:12345678:mapRun:CopyOutStateMachineABCD/4474d22f-4056-30e3-978c-027016edac90:0c17ffd6-e8ad-44c0-a65b-a8b721007241",
  // "ResultFiles":{
  //     "FAILED":[],
  //     "PENDING":[],
  //     "SUCCEEDED":[{"Key":"copy-out-test-working/a6faea86c066cd90/1-objects-to-copy.tsv/0c17ffd6-e8ad-44c0-a65b-a8b721007241/SUCCEEDED_0.json",
  //                   "Size":2887}]}}

  // We read and merge the manifests from all four maps to get the complete set of results.

  const manifestKeys = [
    event.rcloneResultsLarge.manifestKey,
    event.rcloneResultsSmall.manifestKey,
    event.rcloneResultsNeedThawSmall.manifestKey,
    event.rcloneResultsNeedThawLarge.manifestKey,
  ];

  // Arrays to hold the result file keys from all manifests.
  const pendingFiles: any[] = [];
  const failedFiles: any[] = [];
  const succeededFiles: any[] = [];

  // Iterate over each manifest key to fetch and parse the manifest.
  for (const manifestKey of manifestKeys) {
    const getManifestCommand = new GetObjectCommand({
      Bucket: event.workingBucket,
      Key: manifestKey,
    });

    const getManifestResult = await client.send(getManifestCommand);
    if (!getManifestResult.Body) {
      throw new Error("Manifest S3 object Body is undefined");
    }

    const getManifestContent = await getManifestResult.Body.transformToString();

    const manifest = JSON.parse(getManifestContent);

    console.debug(JSON.stringify(manifest, null, 2));

    const resultFiles = manifest["ResultFiles"];

    if (!resultFiles)
      throw new Error(
        "AWS Steps Distributed map manifest.json is missing ResultFiles",
      );

    pendingFiles.push(...resultFiles["PENDING"]);
    failedFiles.push(...resultFiles["FAILED"]);
    succeededFiles.push(...resultFiles["SUCCEEDED"]);
  }

  // Validate manifest shape: all three status buckets must be arrays.
  if (
    !Array.isArray(pendingFiles) ||
    !Array.isArray(failedFiles) ||
    !Array.isArray(succeededFiles)
  )
    throw new Error(
      "AWS Steps Distributed map manifest.json is missing an expected array for PENDING, FAILED or SUCCEEDED",
    );

  // Fail fast if any work is still pending. By the time this Lambda runs,
  // the Distributed Map should have fully completed (no partial results).
  if (pendingFiles.length > 0)
    throw new Error(
      "AWS Steps Distributed map manifest.json indicates there are PENDING results which is not a state we are expecting",
    );

  // This Lambda is only expected to be run when the copy has fully succeeded.
  // Maybe in future we want to handle partial success.
  if (failedFiles.length > 0)
    throw new Error("Copy is meant to succeed - but it had failed results");

  const fileResults: Record<string, FileResult> = {};

  for (const succeededFile of succeededFiles) {
    const getSuccessCommand = new GetObjectCommand({
      Bucket: event.workingBucket,
      Key: succeededFile["Key"],
    });

    const getSuccessResult = await client.send(getSuccessCommand);
    if (!getSuccessResult.Body) {
      throw new Error("Success S3 object Body is undefined");
    }

    const getSuccessContent = await getSuccessResult.Body.transformToString();

    const content = getSuccessContent.trim();
    const rows = content.startsWith("[")
      ? JSON.parse(content)
      : JSON.parse(
          `[${content
            .split(/\r?\n/)
            .filter((l) => l.trim())
            .join(",")}]`,
        );

    for (const row of rows) {
      // NOTE/WARNING: this behaviour is very dependent on rclone and our interpretation
      // of rclone stats - so if things start breaking this is where I would start
      // looking

      const source = row["source"];
      // The name is the basename of the source..

      // Original values
      // const errors: number = rcloneRow["errors"];
      // const lastError: number = rcloneRow["lastError"];
      // const serverSideCopyBytes: number = rcloneRow["serverSideCopyBytes"];
      // const elapsedTime = rcloneRow["elapsedTime"];
      // const totalTransfers = rcloneRow["totalTransfers"];
      // const retryError = rcloneRow["retryError"];

      // Debbuged

      // Is there were any errors during the transfer ?
      const errors: number = row["errors"] ?? 0;

      const name = basename(source);

      // Compute the transfer speed
      const elapsedSeconds = row["elapsedSeconds"] ?? 0;
      const bytesTransferred = row["bytesTransferred"] ?? 0;
      const speedMiBps =
        elapsedSeconds > 0
          ? bytesTransferred / elapsedSeconds / 1024 / 1024
          : 0;

      const destination = row["destination"];
      const lastError: number = row["lastError"];
      const retryError = row["retryError"]; // Not being passed

      // firstly if we have been signalled an error - we need to report that
      if (errors > 0) {
        fileResults[name] = {
          name: name,
          status: "ERROR",
          speed: speedMiBps,
          message: lastError,
          destination: destination,
          bytesTransferred: bytesTransferred,
          elapsedSeconds: elapsedSeconds,
        };
      } else {
        // if we didn't end up transferring anything BUT there was no actual error AND
        // we did a retry - then that probably means the source file didn't exist
        if (bytesTransferred < 1 && retryError) {
          fileResults[name] = {
            name: name,
            status: "ERROR",
            speed: speedMiBps,
            message: "source file did not exist so nothing was transferred",
            destination: destination,
            bytesTransferred: bytesTransferred,
            elapsedSeconds: elapsedSeconds,
          };
        }
        // if we didn't end up transferring anything BUT there was no actual error
        // AND we didn't do any retries then changes are we skipped due to it already
        // being at the destination
        else if (bytesTransferred < 1 && !retryError) {
          fileResults[name] = {
            name: name,
            status: "ALREADYCOPIED",
            speed: speedMiBps,
            message:
              "destination file already exists with same checksum so nothing was transferred",
            destination: destination,
            bytesTransferred: bytesTransferred,
            elapsedSeconds: elapsedSeconds,
          };
        } else {
          fileResults[name] = {
            name: name,
            status: "COPIED",
            speed: speedMiBps,
            message: "",
            destination: destination,
            bytesTransferred: bytesTransferred,
            elapsedSeconds: elapsedSeconds,
          };
        }
      }
    }
  }

  // debug results before we create the CSV
  console.debug(JSON.stringify(fileResults, null, 2));

  const output = stringify(Object.values(fileResults), {
    header: true,
    columns: {
      name: "OBJECTNAME",
      status: "TRANSFERSTATUS",
      speed: "MBPERSEC",
      message: "MESSAGE",
      destination: "DESTINATION",
      bytesTransferred: "BYTESTRANSFERRED",
      elapsedSeconds: "ELAPSEDSECONDS",
      // source: "SOURCE",
      // copyMode: "COPYMODE",
    },
  });

  const putCommand = new PutObjectCommand({
    Bucket: event.destinationBucket,
    Key: `${event.destinationPrefixKey}${event.destinationEndCopyRelativeKey}`,
    Body: output,
  });

  // Determine if we need to generate and store the HTML report(s)
  const includeReport = event.includeCopyReport === true;
  const retainUri = (event.retainCopyReportS3Uri ?? "").trim();
  const retainReport = retainUri.length > 0;

  const csvKey = `${event.destinationPrefixKey}${event.destinationEndCopyRelativeKey}`;
  const htmlKey = csvKey.replace("ENDED_COPY.csv", "copy_report.html");

  if (includeReport || retainReport) {
    // generate the HTML report
    const html = createHtmlReport({
      title: "Copy Results Report",
      records: Object.values(fileResults) as FileResult[],
      destinationBucket: event.destinationBucket,
      destinationFolderKey: event.destinationPrefixKey,
    });

    // 1) Copy to the destination bucket/folder
    if (includeReport) {
      await client.send(
        new PutObjectCommand({
          Bucket: event.destinationBucket,
          Key: htmlKey,
          Body: html,
          ContentType: "text/html; charset=utf-8",
        }),
      );
    }

    // 2) Extra copy to a specific S3 URI (sender retention)
    if (retainReport) {
      const { bucket, key } = parseS3Uri(retainUri);

      if (!bucket || !key) {
        throw new Error(`Invalid retainCopyReportS3Uri: ${retainUri}`);
      }

      await client.send(
        new PutObjectCommand({
          Bucket: bucket,
          Key: key,
          Body: html,
          ContentType: "text/html; charset=utf-8",
        }),
      );
    }
  }
  await client.send(putCommand);
  return output;
}
